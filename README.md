# poetry-tree

Первая домашка была сделана на простом дереве решений, но дальше стало понятно, что это не годится, и я переделал все на модель с трансформером, из домашки по NLP.
Модель осуществляет перевод с русского языка на английский. Трансформер сильно упрощен для ускорения, всего один слой, по одному блоку внимания, и размерность внутренних состояний всего 64. Языковые словари составляются на основе данных только тренировочного датасета.

Данные для обучения, валидации и теста из файла data/data.txt
В параметрах количество эпох обучения указано 2, для быстрой проверки работоспособности модели. Но для хорошего перевода нужно около 10 эпох.

Через Fire организован запуск в commands.py, доступные комманды: train, infer, run_onnx

Вся конфигурация организована через Hydra, конфигурация находится в config/config.yaml

Хранение модели, результатов и прочего организовано через DVC, в гугл-диске https://drive.google.com/drive/folders/1M4Fe8nT36ufluQJB-IRDCHz03aB7iOoc

Логирование обучения модели сделано через MLFlow, выведены 4 графика метрик, сохранен тег с ID текущего коммита, словари с параметрами из Hydra, и полученными во время обучения.

Этап инференса модели по комманде infer проводит полную обработку тестовой части датасета и считает BLEU метрику качества перевода.

Этап инференса модели по комманде run_onnx загружает последнюю сохраненную модель в формате ONNX и производит перевод одного короткого предложения моделью с русского языка на английский.

Код конечно же еще нужно причесывать, т.к. есть дубляжи функций, и прочая фигня.
